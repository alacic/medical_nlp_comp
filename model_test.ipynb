{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from core.utils import (fix_columns, extract_X, extract_y, MultiLabelOverSampler, \n",
    "                        export_result, k_fold, TransProbaTransformer)\n",
    "from core.model import (get_cnn_model, loss_region, loss_type, lr_schedule, opt, early_stop,\n",
    "                        PADDING_SIZE, metric_score, EPOCHS, BATCH_SIZE, metric_score_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 90)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 90, 180)      180000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 90, 180)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 90, 180)      0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 90, 180)      0           spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.not_equal (TFOpLambda)  (None, 90)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 90, 180)      146880      dropout[0][0]                    \n",
      "                                                                 tf.math.not_equal[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 90, 180)      97380       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 90, 180)      129780      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 90, 180)      162180      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 90, 180)      226980      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 90, 180)      291780      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 90, 900)      0           conv1d[0][0]                     \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 90, 900)      3600        tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 90, 900)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 86, 900)      2430900     re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 86, 900)      3600        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 43, 900)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 43, 900)      0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 39, 900)      2430900     re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 900)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 900)          3600        global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 900)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "label (Dense)                   (None, 17)           15317       re_lu_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,122,897\n",
      "Trainable params: 6,117,497\n",
      "Non-trainable params: 5,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_cnn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)\n",
    "test_data_file = \"./data/testA.csv\"\n",
    "train_data_file = \"./data/train.csv\"\n",
    "train_data_file_round_1 = \"./data/track1_round1_train_20210222.csv\"\n",
    "# test_data_file = \"/tcdata/testA.csv\"\n",
    "# train_data_file = \"/tcdata/train.csv\"\n",
    "# train_data_file_round_1 = \"/tcdata/track1_round1_train_20210222.csv\"\n",
    "result_file = \"./result.csv\"\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    train_data_file,\n",
    "    header=None,\n",
    "    names=[\"report_ID\", \"description\", \"labelA\", \"labelB\"],\n",
    ").fillna(\"\").applymap(fix_columns)\n",
    "\n",
    "df_train_round_1 = pd.read_csv(\n",
    "    train_data_file_round_1,\n",
    "    header=None,\n",
    "    names=[\"report_ID\", \"description\", \"labelA\"],\n",
    ").fillna(\"\").applymap(fix_columns)\n",
    "\n",
    "df_train_round_1[\"labelB\"] = \"\"\n",
    "\n",
    "df_train = pd.concat([df_train, df_train_round_1])\n",
    "\n",
    "df_train[\"X\"] = df_train.description.apply(extract_X)\n",
    "df_train[\"X_len\"] = df_train.X.str.len()\n",
    "df_train = df_train.loc[df_train.X_len.between(5, 90)]\n",
    "\n",
    "condi_value = (df_train.labelB!=\"\")\n",
    "df_train_labelB = df_train.loc[condi_value]\n",
    "df_train.index = range(len(df_train))\n",
    "df_train_labelB.index = range(len(df_train_labelB))\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    test_data_file,\n",
    "    header=None,\n",
    "    names=[\"report_ID\", \"description\"],\n",
    ").fillna(\"\").applymap(fix_columns)\n",
    "\n",
    "print(f\"basic info {'=' * 15}\")\n",
    "print(f\"train_size {len(df_train)}\")\n",
    "print(f\"df_train_labelB_size {len(df_train_labelB)}\")\n",
    "print(f\"test_size {len(df_test)}\")\n",
    "print(f\"basic info {'=' * 15}\\n\\n\")\n",
    "\n",
    "df_test[\"X\"] = df_test.description.apply(extract_X)\n",
    "\n",
    "trainX = sequence.pad_sequences(df_train.X, PADDING_SIZE, padding=\"post\")\n",
    "trainX_labelB = sequence.pad_sequences(df_train_labelB.X, PADDING_SIZE, padding=\"post\")\n",
    "\n",
    "train_yA = np.array(df_train.labelA.apply(extract_y).tolist())\n",
    "train_yB = np.array(df_train_labelB.labelB.apply(extract_y, args=(12, )).tolist())\n",
    "\n",
    "testX = sequence.pad_sequences(df_test.X, PADDING_SIZE, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "textCNN_models_labelB = []\n",
    "historys = []\n",
    "for j, ((trainX_i, valiX_i), (train_yB_i, vali_yB_i)) in enumerate(k_fold(trainX_labelB, train_yB, random_state=0)):\n",
    "    print(f\"{j:=^90d}\")\n",
    "    model_i = get_cnn_model(output_shape=12)\n",
    "    model_i.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss=loss_region, \n",
    "        metrics=[loss_type]\n",
    "    )\n",
    "    \n",
    "    history_i = model_i.fit(\n",
    "        trainX_i,\n",
    "        train_yB_i, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(valiX_i, vali_yB_i),\n",
    "    )\n",
    "    \n",
    "    textCNN_models_labelB.append(model_i)\n",
    "    historys.append(history_i)\n",
    "    print(f\"{'=' * 90}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "textCNN_models_labelA = []\n",
    "historys = []\n",
    "for j, ((trainX_i, valiX_i), (train_yA_i, vali_yA_i)) in enumerate(k_fold(trainX, train_yA, random_state=0)):\n",
    "    print(f\"{j:=^90d}\")\n",
    "    model_i = get_cnn_model(output_shape=17)\n",
    "    model_i.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss=loss_region, \n",
    "        metrics=[loss_region]\n",
    "    )\n",
    "    \n",
    "    history_i = model_i.fit(\n",
    "        trainX_i,\n",
    "        train_yA_i, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(valiX_i, vali_yA_i),\n",
    "    )\n",
    "    \n",
    "    textCNN_models_labelA.append(model_i)\n",
    "    historys.append(history_i)\n",
    "    print(f\"{'=' * 90}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for m, mB in zip(textCNN_models_labelA, textCNN_models_labelB):\n",
    "    yhat_A_i = m.predict(testX)\n",
    "    yhat_B_i = mB.predict(testX)\n",
    "    results.append(np.c_[yhat_A_i, yhat_B_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reduce(add, results) / n_splits\n",
    "export_result(result_file, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
