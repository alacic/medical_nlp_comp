{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from core.utils import (fix_columns, extract_X, extract_y, MultiLabelOverSampler, \n",
    "                        export_result, k_fold, TransProbaTransformer)\n",
    "from core.model import (get_cnn_model, loss_region, loss_type, lr_schedule, opt, early_stop,\n",
    "                        PADDING_SIZE, metric_score, EPOCHS, BATCH_SIZE, metric_score_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_cnn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)\n",
    "test_data_file = \"./data/testA.csv\"\n",
    "train_data_file = \"./data/train.csv\"\n",
    "train_data_file_round_1 = \"./data/track1_round1_train_20210222.csv\"\n",
    "# test_data_file = \"/tcdata/testA.csv\"\n",
    "# train_data_file = \"/tcdata/train.csv\"\n",
    "# train_data_file_round_1 = \"/tcdata/track1_round1_train_20210222.csv\"\n",
    "result_file = \"./result.csv\"\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    train_data_file,\n",
    "    header=None,\n",
    "    names=[\"report_ID\", \"description\", \"labelA\", \"labelB\"],\n",
    ").fillna(\"\").applymap(fix_columns)\n",
    "\n",
    "df_train_round_1 = pd.read_csv(\n",
    "    train_data_file_round_1,\n",
    "    header=None,\n",
    "    names=[\"report_ID\", \"description\", \"labelA\"],\n",
    ").fillna(\"\").applymap(fix_columns)\n",
    "\n",
    "df_train_round_1[\"labelB\"] = \"\"\n",
    "\n",
    "df_train = pd.concat([df_train, df_train_round_1])\n",
    "\n",
    "df_train[\"X\"] = df_train.description.apply(extract_X)\n",
    "df_train[\"X_len\"] = df_train.X.str.len()\n",
    "df_train = df_train.loc[df_train.X_len.between(5, 90)]\n",
    "\n",
    "condi_value = (df_train.labelB!=\"\")\n",
    "df_train_labelB = df_train.loc[condi_value]\n",
    "df_train.index = range(len(df_train))\n",
    "df_train_labelB.index = range(len(df_train_labelB))\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    test_data_file,\n",
    "    header=None,\n",
    "    names=[\"report_ID\", \"description\"],\n",
    ").fillna(\"\").applymap(fix_columns)\n",
    "\n",
    "print(f\"basic info {'=' * 15}\")\n",
    "print(f\"train_size {len(df_train)}\")\n",
    "print(f\"df_train_labelB_size {len(df_train_labelB)}\")\n",
    "print(f\"test_size {len(df_test)}\")\n",
    "print(f\"basic info {'=' * 15}\\n\\n\")\n",
    "\n",
    "df_test[\"X\"] = df_test.description.apply(extract_X)\n",
    "\n",
    "trainX = sequence.pad_sequences(df_train.X, PADDING_SIZE, padding=\"post\")\n",
    "trainX_labelB = sequence.pad_sequences(df_train_labelB.X, PADDING_SIZE, padding=\"post\")\n",
    "\n",
    "train_yA = np.array(df_train.labelA.apply(extract_y).tolist())\n",
    "train_yB = np.array(df_train_labelB.labelB.apply(extract_y, args=(12, )).tolist())\n",
    "\n",
    "testX = sequence.pad_sequences(df_test.X, PADDING_SIZE, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "textCNN_models_labelB = []\n",
    "historys = []\n",
    "for j, ((trainX_i, valiX_i), (train_yB_i, vali_yB_i)) in enumerate(k_fold(trainX_labelB, train_yB, random_state=0)):\n",
    "    print(f\"{j:=^90d}\")\n",
    "    model_i = get_cnn_model(output_shape=12)\n",
    "    model_i.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss=loss_region, \n",
    "        metrics=[loss_type]\n",
    "    )\n",
    "    \n",
    "    history_i = model_i.fit(\n",
    "        trainX_i,\n",
    "        train_yB_i, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(valiX_i, vali_yB_i),\n",
    "    )\n",
    "    \n",
    "    textCNN_models_labelB.append(model_i)\n",
    "    historys.append(history_i)\n",
    "    print(f\"{'=' * 90}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "textCNN_models_labelA = []\n",
    "historys = []\n",
    "for j, ((trainX_i, valiX_i), (train_yA_i, vali_yA_i)) in enumerate(k_fold(trainX, train_yA, random_state=0)):\n",
    "    print(f\"{j:=^90d}\")\n",
    "    model_i = get_cnn_model(output_shape=17)\n",
    "    model_i.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss=loss_region, \n",
    "        metrics=[loss_region]\n",
    "    )\n",
    "    \n",
    "    history_i = model_i.fit(\n",
    "        trainX_i,\n",
    "        train_yA_i, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(valiX_i, vali_yA_i),\n",
    "    )\n",
    "    \n",
    "    textCNN_models_labelA.append(model_i)\n",
    "    historys.append(history_i)\n",
    "    print(f\"{'=' * 90}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for m, mB in zip(textCNN_models_labelA, textCNN_models_labelB):\n",
    "    yhat_A_i = m.predict(testX)\n",
    "    yhat_B_i = mB.predict(testX)\n",
    "    results.append(np.c_[yhat_A_i, yhat_B_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reduce(add, results) / n_splits\n",
    "export_result(result_file, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
